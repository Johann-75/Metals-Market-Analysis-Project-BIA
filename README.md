# Precious Metals Price Intelligence System

A comprehensive, production-grade system to track, store, and analyze precious metal prices (Gold, Silver, Platinum, Palladium) using the **metals.dev** API, **Supabase** (PostgreSQL), and a **Streamlit** dashboard.

## ğŸ—ï¸ Architecture

The system follows a modular 3-tier architecture:

1.  **Data Ingestion (ETL) Layer**:
    *   **Source**: `metals.dev` API (Spot, MCX, LBMA prices).
    *   **Logic**: Python script (`backend/etl_pipeline.py`) extracts data, transforms it into a Star Schema format, and loads it into Supabase.
    *   **Automation**: `APScheduler` (`backend/scheduler.py`) fetches data every hour.

2.  **Data Warehouse (PostgreSQL/Supabase)**:
    *   **Schema**: Star Schema for analytical performance.
        *   **Fact Table**: `fact_metal_prices` (price, currency, unit, foreign keys).
        *   **Dimension Tables**: `dim_metal` (Name), `dim_market` (Spot/MCX/LBMA), `dim_time` (Date parts, hierarchy).

3.  **Analytics & Visualization Layer**:
    *   **Analytics Engine**: Python module (`backend/analytics.py`) calculates:
        *   Daily % Change
        *   Moving Averages (7-day, 30-day)
        *   Correlation Matrices
        *   Volatility/Spike Detection (>3% variance)
    *   **Dashboard**: Streamlit app (`dashboard/app.py`) provides:
        *   Dark-themed UI
        *   Interactive Price Charts (Plotly)
        *   Correlation Heatmaps
        *   Volatility Alerts

## ğŸš€ Setup & Installation

### Prerequisites
*   Python 3.8+
*   Supabase Account (for PostgreSQL)
*   metals.dev API Key (Free tier sufficient for Spot prices)

### 1. Clone & Install Dependencies
```bash
# Clone repository
git clone <repo-url>
cd metals-intelligence

# Install Python packages
pip install -r requirements.txt
```

### 2. Configure Environment
Create a `.env` file in the root directory:
```bash
METALS_DEV_API_KEY=your_api_key_here
SUPABASE_URL=your_supabase_project_url
SUPABASE_KEY=your_supabase_service_role_key
```

### 3. Database Setup
Run the SQL script in your Supabase SQL Editor to create the schema:
*   File: `database/01_create_tables.sql`

## ğŸƒ Usage

### Run ETL Pipeline (Manually)
Fetch the latest prices immediately:
```bash
python backend/etl_pipeline.py
```

### Run Scheduler (Background)
Start the automated fetcher (runs every 1 hour):
```bash
python -m backend.scheduler
```
*Note: In production, use a process manager like Supervisor or Docker.*

### Launch Dashboard
Start the analytics interface:
```bash
streamlit run dashboard/app.py
```
Access at `http://localhost:8501`.

## ğŸ“ Project Structure
```
/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ analytics.py       # Metrics calculation logic
â”‚   â”œâ”€â”€ etl_pipeline.py    # API fetch & DB insert logic
â”‚   â””â”€â”€ scheduler.py       # Automation runner
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ app.py             # Main Streamlit dashboard
â”‚   â””â”€â”€ utils.py           # Dashboard helper functions
â”œâ”€â”€ database/
â”‚   â””â”€â”€ 01_create_tables.sql # SQL Schema definition
â”œâ”€â”€ .env                   # Secrets (Excluded from Git)
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md              # Documentation
```

## ğŸ› ï¸ Tech Stack
*   **Language**: Python
*   **Database**: PostgreSQL (Supabase)
*   **API**: metals.dev
*   **Visualization**: Streamlit, Plotly
*   **Scheduling**: APScheduler
*   **Data Processing**: Pandas

---
*Generated by Antigravity*
